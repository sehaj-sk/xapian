Since the QueryParser has a hand-written lexer, and a Lemon generated parser,
I figured that it would be better to control the Lexer so as to recover from
errors rather than trying it at the Parser's end.

Following details my information as to what should the parse of error
cases be as well as how to overcome it. The numbering used is the same as
report/summary.rst

1. Due to tokens BRA and KET

    a. WHAT:  If the opening bracket character '(' comes after a word without a
              whitespace then, lexer ignores it. But the corresponding closing
              bracket ')' is detected. Since the only grammar production
              corresponding to BRA and KET is compound_term ::= BRA expr
              KET. , thus every BRA requires KET and every KET requires
              a BRA.

       PARSE: It would be most appropriate if these characters were not
              neglected. Thus if the token BRA could be generated corresponding
              to character '(' irrespective of whether it comes after a
              whitespace or not, thne it would serve the purpose. Lucene also
              does it this way. The file showing these changes is present here-
              report/detect_bra.diff .

       HOW:   In the present syntax of queryparser.lemony (line 888), are the
              conditions present for the character '(' to be treated as the
              token BRA. Removing these conditions should be appropriate. This
              shall also make sure that parse errors of type 1.e don't occur.

       RESULTS: I tried this on queryparsertest.cc and the results very
                successful. Around 60 (57-58 to be exact) parse errors
                from real world queries were eliminated by making this
                change. No new parse errors were generated in other sections
                of queryparsertest.cc, but in the real-world queries portion,
                around 10 new queries generated parse errors. But well,
                they deserved it (i guess). Following are those queries :

                    onmouseover=setPointer(this
                    line 8: syntax error near unexpected token
                    `kernel_thread(f'
                    execCommand('inserthorizontalrule'
                    select max( mysql
                    leejow(saait
                    [php] date( min
                    header("Content-Disposition: attachment;
                    "header("Content-Disposition: attachment;"
                    window.open( scrollbar
                    document.write(ssg(" html
                    Novell This pentium class machine (or greater) lacks
                    some required CPU feature(s

                These queries would be grouped with the parse error of
                unmatching brackets (mentioned in 1.c).



    b. WHAT:  If the characters '(' or ')' are used to represent emoticon or
              emoticon like objects, then also it leads to parse errors since
              the grammar production compound_term ::= BRA expr KET. is not
              satisfied. This situation can be really common if the data like
              Twitter feeds or Facebook status or anything similar to this
              is used.

       PARSE: It would be appropriate to make the query free of emoticons
              before passing it to lexer or parser. This shall avoid making
              the lexer complicated and adding it as a sort of layer, before
              passing to lexer would make it easier to turn on or off via
              appropriate flags.

       HOW:   Have written a small emoticon extractor class for this purpose.
              This class is like a mini-lexer in itself which ains to find 
              emoticons in the given string.
              Have tried to make the methods in such a way, that it shall
              be easy to change the library/definition of emoticons, (if
              required). The appropriate methods from this class can be added
              to queryparser.lemony. 
              I have made a demo program which uses this class and is present in 
              report/emocticon.cc . 
              This program takes a query, and returns a new query from which
              emoticons have been removed/extracted, along with the number
              and details of the emoticons extracted.
              A Few sample testcases are present in the end of emoticon.cc as well.

       RESULTS: Have tried this on real world queries from queryparsertest.cc
                as well as some of my own made queries, and the results were
                good. However there are some questions which need answers. 

               Main Decisions To Be Taken To Modify the present version
               of code:

                   1. Whether to have the condition of a whitespace as a must
                   before an emoticon OR allow either whitespace or non-word
                   character(s) before it. Presently it support the second
                   one i.e. either whitespace or non-word character(s).

                       Pros of present strategy :
                           Gives the feature of detecting many more emoticons,
                           thus expanding our library though keeing the
                           simple present version of code. Reason behind
                           this is that all emoticons have the basic structure
                           same as used in the program. The (possible)
                           variation(s) comes when characters (non-word) are
                           either prefixed or suffixed to the emoticon. For
                           example: the (sort of imaginary) emoticon "<:-)"
                           can be detected as the emoticon ":-)", but if
                           we make the presence of whitespace necessary,
                           then such emoticons (or other emoticons such that
                           there is some non-word character suffixed to them)
                           would not be detected.

                       Cons of the present strategy :
                           May lead to detection of some emoticons
                           which probably should not have been
                           detected as emoticons. For example, the
                           query "if(usercode==passcode==)" (present in
                           queryparsrtest.cc, only this query from real world
                           queries in queryparsertest.cc gave this error),
                           gives "=)" as an emoticon, whereas it should not
                           have done so. If we made whitespace necessary,
                           then we could prevent ourselves from such
                           conditions. Also, there are some (or maybe many)
                           places like chat messengers (yahoo etc.) where it
                           is must that a whitespace is present before the
                           emoticon. Thus the whitespace condition would be
                           consistent with such (pseudo) conventions.

                       Solution:
                           We may either choose a particular strategy or
                           leave it to the user to choose what strategy the
                           user want by choosing appropriate flags.


                   2. To only detect emoticons related to characters ')' ,
                   '(' (this means limiting the set of characters in MOUTH
                   state of emoticon), since they are the ones because of
                   which parser actually generates error OR to make the
                   set of characters in MOUTH state to be more (as in the
                   present version of code).

                       Pros and cons of the present strategy:
                           For characters like 'p', 'P', 'd', 'D', the
                           corresponding emoticons like ":-D", ":P" etc. could
                           be detected which would mean that these characters
                           won't generate the TERM token thus we won't have
                           to search for things like TERM("P") or TERM("D"),
                           so that would save a lot of time in search. Though
                           characters like '/', '\' corresponding to emoticons
                           like ":-/" etc. could be dropped from the set of
                           MOUTH characters sincw these don't lead to parse
                           errors, but the con for doing this would be that
                           it would limit the library of emoticon. But here
                           limiting the library of emoticon makes a difference
                           or not depends on whether the emoticon_extractor
                           is to be used just to prevent the parse errors
                           or to also provide the feature of detecting what
                           and how many emoticons are present ?

                       Solution:
                           We may decide what all characters (for MOUTH
                           etc. )to go with OR we can make the characters, in
                           present version as default ( after some modification
                           to those groups, IF reuired) and give the user
                           the freedom to add/remove and define his/her own
                           set of characters for all or some options- MOUTH,
                           EYES, SUB-EYES, NOSE.


                   3. In the present version of code, I have made the
                   condition that in a string, emoticon is detected only if it
                   is NOT followed by word character(s) (as detected via
                   is_wordchar(*it)). Is this right or does thing condition
                   require modification? This condition makes sure that
                   in strings like "new :Parser", ":P" is not detected as
                   an emoticon.



    c. WHAT:  If there is a BRA or a KET and not both. (different from case
              1.a as there, both were present but the token BRA was neglected
              as it just followed the word, without a whitespace). Thus this
              case corresponds to the unmatched brackets.

       PARSE: There are two options. Either to drop off the unmatching brackets
              or to add in the closing brackets to make the brackets completed.

       HOW:   After quite a bit of thinking, I went with adding bracket(s) if the
              brackets aren't matched. This one was really- really effective.
              The changes are present here - report/unmatched_brackets.diff

       RESULTS: Tried this on queryparsertest.cc and the results were
                really great. The most important thing is that other
                alternative solutions to deal with this error, would be
                quite a bit of overhead, whereas this method is effective
                and really efficient. Around 75-80 parse errors from real
                world queries were fixed from this change. Also, this can
                almost everytime also handle the errors caused because of
                emoticons. This change is like a head of all the errors
                caused due to tokens BRA and KET.



    d. WHAT:  If the opening and closing brackets (BRA and KET), have only
              characters in between them, then those characters are
              ignored. Thus tokens recieved by parser are BRA and then KET
              without any other token in between and this leads to parser
              error. But note that if we have a query like "xapian ()
              google", neither the token BRA nor the token KET is produced
              (since nothing between them, thus both are ignored by the lexer).

       PARSE: The appropriate method to deal with this error will be that
              under such situations, the tokens BRA and KET are not passed
              by lexer to the parser.

       HOW:   Making the lexer ignore such '(' and ')' characters, which have
              only non-word characters in between them. I made one-two small
              new methods for doing this. They are present here -
              report/pseudo_emptybrackets.diff

       RESULTS: The results of trying this on queryparsertest.cc were
                promising. All the queries which were not able to get parsed
                because of this, were able to get parsed after this change.
   
   

    e. WHAT: There was also a single case where the opening bracket character
             '(' comes just after some other character (non-word character),
             then also the problem rises. The one single query correspoding
             to this was - "Unable to find libgd.(a|so) anywhere" Here the
             lexer doesn't passes any BRA token to parser since the character
             '.' comes before it. But the token KET is passed, thus leading
             to error since there is no BRA corresponding to this KET.
             
       As mentioed in 1.a, doing that would take care of this error as well.





2. Due to token QUOTE.

    a. WHAT:  Empty Quotes "" and Quotes with only characters (Non-word
              characters) between them also lead to quite a lot of errors. If
              there are only character(s) (Non-word character) between the
              quotes, then those characters are neglected, leading to parse
              error since the only grammar productions related to quotes
              is compound_term ::= QUOTE phrase QUOTE. , where phrase can't
              be null.

       PARSE: Similar to 1.d, in such cases the token QUOTE should not be
              sent by the lexer to the parser. The change is present here -
              report/pseudo_emptyquotes.diff
   
       HOW: Similar to 1.d 

       RESULTS: Like 1.d, this change was effective as tested for
                queryparsertest.cc


    b.  WHAT:  If there is an opening Quote but no corresponding closing quote,
               then that also lead to parse error. Thus this case corresponds
               to unmatches quotes, similar to the error of unmatched brackets
               of 1.c. There were about three queries with such error.

        PARSE: Like 1.c, here we have two options, to either drop the
               non-matching quotes or to fill in quote so as to get them
               matched.

        HOW: If we do the detection of empty 2.a type of errors, then these
             types of error don't occur since the lexer checks that
             if at the end of the query string, we are in IN_QUOTES or
             IN_PREFIXED_QUOTES, then it passes a token QUOTE to the parser
             (line 1318, queryparser.lemony)

     
     
3. Due to tokens LOVE and HATE.

    WHAT:  Another kind of parse errors were related to the tokens LOVE and
           HATE. There must be atleast a term after the token LOVE or HATE,
           since the corresponding grammar productions are prob ::= LOVE
           term. and prob ::= stop_prob LOVE term. and similar rules for
           the token HATE. Thus if there is nothing after these tokens or if
           there are just character(s) (Non-word character), then that also
           lead to parse error (since non-word characters are simply ignored
           by the lexer).

    PARSE: Similar to 1.d and 2.a, in this case, the token LOVE/HATE should
           not be generated.

    HOW:   Making the lexer ignore '-'/'+', if only non-word characters come
           after these characters before a whitespace comes. Here is the
           change - report/pseudo_lovehate.diff .

    RESULT: Like 1.d and 2.a, this change was effective when tested on
            queryparsertest.cc.  All the queries which were not able to
            get parsed because of this, were able to get parsed after this
            addition.
            And there was one query that gave parse errors which was not
            giving the parse error earlier. That query was -
            "putStr (map (\\x -> chr (round (21/2 * x^3 - 92 * x^2 + 503/2 *
            x - 105))) [1..4])"
            This query showed that in the present syntax, in queries like
            "word1 -(some non-word characters) word2", for example, in query,
            "xapian -# google" the '-' will be treated as HATE, whereas it
            should not be treated like that.
            This method would also help to remove this bug.



    
4. Due to token NEAR.

    WHAT:  There were some three - four queries which generated parse error
           because of NEAR operator. The reason for that is that according to
           the grammar rules - near_expr ::= TERM NEAR TERM. and near_expr
           ::= near_expr NEAR TERM. , on the both sides of NEAR operator,
           there can only be TERM and nothing else.

    PARSE: Such a query can't be a NEAR query. So I guess, such queries
           can't be parsed, unless we ignore the NEAR operator.


5. 

    WHAT: If we have a HATE query between the brackets then it leads to
          parse error but if we have the LOVE query between the brackets then
          it doesn't ! For example, the query (+xapian) parses without giving
          error but the query (-xapian) does not get parsed and gives error.
          The reason being we can't just hate term(s).

    HOW:  Currently, have set the error message along with a suggesetion. Here
          is the change - report/near_failure.diff

    RESULT: Tested on queryparsertest.cc. When such kind of parse errors
            occurs, the parser gives the mentioned error message.



After applying all the changes mentioned above, there are only 6-7 real-world
queries which don't get parsed (including the queries who give parse errors
because of NEAR operator, for which I have set the message) while trying to
parse by disabling the reparse with flags off code. It came down to this
number from 130, with just these small-small changes/additions. Also, the
other testcases present in queryparsertest.cc seems to be working fine with
these changes.

Here is the file where all the above mentioned changes (except emoticon
detector) have been applied to queryparser.lemony -
report/allchanges_queryparser.diff

Note that at present in the above diff file, these changes are done without
setting any extra flags so as to turn on/off these changes. It will be
appropriate to put these changes under one or more flags, and first try to
parse the query as such without these changes. If the parsing fail, then we
can turn on these flag(s) and try to parse the query again.

